<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://penseemdados.github.io/bayesian_optimization_algorithm_book/feed.xml" rel="self" type="application/atom+xml" /><link href="https://penseemdados.github.io/bayesian_optimization_algorithm_book/" rel="alternate" type="text/html" /><updated>2024-09-28T17:47:12-03:00</updated><id>https://penseemdados.github.io/bayesian_optimization_algorithm_book/feed.xml</id><title type="html">Pense Como um Cientista de Dados</title><subtitle>Este site acompanha o livro &quot;Pense Como um Cientista de Dados - Modelos de Árvore e Otimização Bayesiana para Classificação Binária&quot;, com exemplos práticos aplicados ao mundo dos negócios.</subtitle><entry><title type="html">Apresentacao</title><link href="https://penseemdados.github.io/bayesian_optimization_algorithm_book/2024/09/25/apresentacao.html" rel="alternate" type="text/html" title="Apresentacao" /><published>2024-09-25T00:00:00-03:00</published><updated>2024-09-25T00:00:00-03:00</updated><id>https://penseemdados.github.io/bayesian_optimization_algorithm_book/2024/09/25/apresentacao</id><content type="html" xml:base="https://penseemdados.github.io/bayesian_optimization_algorithm_book/2024/09/25/apresentacao.html"><![CDATA[<p>Bem-vindo a uma jornada prática pelo mundo da inteligência artificial aplicada aos negócios! Se você já se pegou pensando em como aqueles modelos de <em>machine learning</em> podem, de fato, resolver problemas reais do dia a dia empresarial, este livro é para você.</p>

<p>Antes de começarmos, um aviso amigável: este livro não é um curso de <em>machine learning</em> nem um manual de <strong>Python</strong>. Vamos assumir que você já sabe o que é um algoritmo, já escreveu algumas linhas de código em <strong>Python</strong> e entende os conceitos básicos de aprendizado de máquina. Se você sabe que <em>overfitting</em> não é um novo treino na academia e que <code class="language-plaintext highlighter-rouge">pandas</code> não são apenas aqueles ursos chinenes que comem bambu, estamos no caminho certo!</p>

<p>Nosso foco aqui é a aplicação prática. Vamos pegar um algoritmo originalmente desenvolvido para prever doenças cardíacas e mostrar como adaptá-lo para resolver seis problemas comuns no mundo dos negócios. Sim, seis! Porque acreditamos que a melhor maneira de aprender é colocando a mão na massa, ou melhor, no teclado.</p>

<p>Ao longo dos capítulos, vamos explorar situações como prever a inadimplência de um cliente, detectar fraudes em transações financeiras e até mesmo antecipar se aquele cliente fiel está prestes a abandonar o barco. Tudo isso utilizando técnicas avançadas de otimização de modelos de <em>ensemble</em> com <strong>Otimização Bayesiana</strong> e adicionando uma pitada de interpretabilidade com valores SHAP.</p>

<p>E não se preocupe, apesar dos temas complexos, vamos manter a conversa leve e descontraída. Afinal, quem disse que não podemos aprender e nos divertir ao mesmo tempo? Como diria Marcus du Sautoy, a matemática (e, por extensão, a ciência de dados) é uma aventura repleta de descobertas e surpresas.</p>

<p>Prepare-se para decifrar mistérios, solucionar enigmas de negócios e, quem sabe, até impressionar aquele colega que sempre fala em jargões complicados. Vamos descomplicar juntos o mundo do <em>machine learning</em> aplicado!</p>

<p>Então, ajeite a cadeira, prepare o ambiente de desenvolvimento e vamos começar essa jornada rumo à aplicação prática e eficiente de algoritmos de aprendizado de máquina nos negócios.</p>

<h2 id="o-artigo-que-inspirou-nossa-jornada">O Artigo que Inspirou Nossa Jornada</h2>
<p>Tudo começou com o artigo <a href="https://www.mdpi.com/2078-2489/15/7/394">Optimized Ensemble Learning Approach with Explainable AI for Improved Heart Disease Prediction</a>[1], publicado em junho de 2024. Não se assuste com o título complicado! Embora não seja um assunto simples e trivial, com um pouco de esforço e dedicação, você verá que é perfeitamente compreensível e, mais importante, aplicável a problemas reais de negócios. Este trabalho explora o uso da <strong>Otimização Bayesiana</strong> para ajustar hiperparâmetros em modelos de <em>ensemble</em>, como <strong>Random Forest</strong>, <strong>AdaBoost</strong> e <strong>XGBoost</strong>, além de empregar valores <strong>SHAP</strong> para interpretar as decisões do modelo.</p>

<p>Em termos simples, eles encontraram uma forma de deixar modelos já poderosos ainda melhores e, de quebra, mais transparentes. E aqui entre nós, quem não quer um modelo que além de eficiente, consegue explicar suas próprias decisões? É quase como ter um GPS que, além de te mostrar o caminho, ainda te conta as histórias dos lugares por onde você passa.</p>

<h2 id="ferramentas-que-vamos-utilizar">Ferramentas que Vamos Utilizar</h2>
<p>Para acompanhar os exemplos e colocar em prática o que vamos aprender, usaremos o <strong>Jupyter Notebook</strong>. Se você ainda não é amigo dessa ferramenta, prepare-se para conhecê-la melhor — ela será sua companheira fiel nesta jornada.</p>

<p>Agora, se você prefere a nuvem e não quer se preocupar com instalações, pode replicar todos os exercícios no <strong>Google Colab</strong>. Basta ter uma conta <strong>Google</strong>, e pronto: todos os recursos estarão ao seu alcance, sem precisar instalar nada.</p>

<p>Mas se você é do tipo que gosta de ter tudo rodando na sua própria máquina, recomendamos a instalação do <strong>Anaconda</strong>. Ele já vem com o <strong>Jupyter Notebook</strong> e todas as bibliotecas necessárias, como <code class="language-plaintext highlighter-rouge">numpy</code>, <code class="language-plaintext highlighter-rouge">pandas</code>, <code class="language-plaintext highlighter-rouge">scikit-learn</code>, <code class="language-plaintext highlighter-rouge">xgboost</code>, <code class="language-plaintext highlighter-rouge">shap</code> e muito mais. É como um pacote completo, tipo aquelas cestas de café da manhã que vêm com tudo o que você precisa para começar o dia feliz.</p>

<h2 id="o-que-esperar-desta-jornada">O Que Esperar Desta Jornada</h2>
<p>Ao longo dos capítulos, vamos:</p>

<ul>
  <li>
    <p><strong>Explorar o algoritmo original</strong>: Entender como o artigo inspirador abordou a previsão de doenças cardíacas usando modelos de <em>ensemble</em> otimizados e interpretáveis.</p>
  </li>
  <li>
    <p><strong>Adaptar e aplicar o algoritmo a diferentes problemas de negócios</strong>: Mostrar como podemos pegar essa base sólida e moldá-la para resolver desafios em diversas áreas.</p>
  </li>
  <li>
    <p><strong>Utilizar técnicas avançadas de otimização de hiperparâmetros com Otimização Bayesiana</strong>: Porque ficar testando combinação por combinação é tão empolgante quanto assistir tinta secar.</p>
  </li>
  <li>
    <p><strong>Tornar nossos modelos interpretáveis usando valores SHAP</strong>: Afinal, queremos saber não só o resultado, mas também o “porquê” por trás dele.</p>
  </li>
  <li>
    <p><strong>Escrever muito código em Python</strong>: E quando dizemos muito, queremos dizer o suficiente para você se sentir um verdadeiro ninja dos notebooks.</p>
  </li>
</ul>

<p>Tudo isso de forma prática, objetiva e, esperamos, divertida. Afinal, aprender não precisa ser chato. <strong>Python</strong> é uma linguagem, e queremos que você se torne fluente nela — ou pelo menos saiba pedir um café e perguntar onde fica o banheiro.</p>

<h2 id="preparando-o-ambiente">Preparando o Ambiente</h2>
<p>Antes de começarmos, certifique-se de que tem tudo o que precisa:</p>

<ul>
  <li>
    <p><strong>Python 3.x</strong> instalado (se estiver usando <strong>Anaconda</strong>, já está incluso).</p>
  </li>
  <li>
    <p><strong>Jupyter Notebook</strong> ou acesso ao <strong>Google Colab</strong>.</p>
  </li>
  <li>
    <p>As bibliotecas necessárias, que iremos instalar e importar ao longo dos capítulos.</p>
  </li>
</ul>

<p>Se optar por usar o <strong>Google Colab</strong>, basta acessar <code class="language-plaintext highlighter-rouge">colab.research.google.com</code> e fazer <em>login</em> com sua conta <strong>Google</strong>. Todos os notebooks podem ser executados diretamente na nuvem, sem necessidade de instalação.</p>

<p>Caso prefira trabalhar localmente, recomendamos a instalação do <strong>Anaconda</strong>, que pode ser baixado em <code class="language-plaintext highlighter-rouge">www.anaconda.com</code>. Ele já vem com o <strong>Jupyter Notebook</strong> e um monte de bibliotecas úteis. É como aquele canivete suíço que todo aventureiro precisa ter.</p>

<h2 id="vamos-começar">Vamos Começar?</h2>
<p>Se você está pronto para aplicar algoritmos poderosos a problemas reais e entender cada passo do caminho, então vamos em frente. Ajuste sua cadeira, prepare o ambiente de desenvolvimento, e vamos iniciar essa jornada rumo à aplicação prática e eficiente de algoritmos de aprendizado de máquina nos negócios.</p>

<p>Ah, e não esqueça de trazer seu senso de humor. Ele pode não ser obrigatório, mas definitivamente torna a viagem mais agradável!</p>

<p><strong>Nota</strong>: Os <em>notebooks</em> completos com os códigos utilizados ao longo do livro estão disponíveis no nosso <strong>GitHub</strong>. Recomendamos que você explore e reproduza todos os passos das análises por si mesmo!</p>

<p>[1] Mienye, Ibomoiye Domor, and Nobert Jere, “Optimized Ensemble Learning Approach with Explainable AI for Improved Heart Disease Prediction,” Information, vol. 15, no. 7, p. 394, 2024. Disponível em: https://www.mdpi.com/2078-2489/15/7/394</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Bem-vindo a uma jornada prática pelo mundo da inteligência artificial aplicada aos negócios! Se você já se pegou pensando em como aqueles modelos de machine learning podem, de fato, resolver problemas reais do dia a dia empresarial, este livro é para você.]]></summary></entry><entry><title type="html">Capitulo 01</title><link href="https://penseemdados.github.io/bayesian_optimization_algorithm_book/2024/09/25/capitulo-01.html" rel="alternate" type="text/html" title="Capitulo 01" /><published>2024-09-25T00:00:00-03:00</published><updated>2024-09-25T00:00:00-03:00</updated><id>https://penseemdados.github.io/bayesian_optimization_algorithm_book/2024/09/25/capitulo-01</id><content type="html" xml:base="https://penseemdados.github.io/bayesian_optimization_algorithm_book/2024/09/25/capitulo-01.html"><![CDATA[<p>Aqui, vamos mergulhar no algoritmo que deu origem a toda essa aventura. Sim, estamos falando daquele artigo com título complicado: “Optimized Ensemble Learning Approach with Explainable AI for Improved Heart Disease Prediction”, publicado em junho de 2024. Embora não seja um assunto trivial, com um pouco de esforço e dedicação, você verá que é perfeitamente compreensível e, mais importante, aplicável a problemas reais de negócios.</p>

<h2 id="entendendo-o-contexto">Entendendo o Contexto</h2>
<p>O artigo aborda um problema altamente relevante: a previsão de doenças cardíacas usando aprendizado de máquina. Os autores propõem uma abordagem inovadora, que combina modelos de <strong>ensemble</strong> otimizados com técnicas de interpretabilidade, com o objetivo não apenas de melhorar a precisão, mas também de entender os fatores que mais influenciam as previsões. Ou seja, não basta saber se uma pessoa tem uma doença cardíaca, é fundamental entender o “porquê” por trás dessa previsão.</p>

<p>E por que isso é importante? Porque, seja na saúde, nos negócios ou em qualquer outro contexto, as previsões baseadas em dados precisam ser confiáveis e explicáveis. E essa combinação de alta performance e interpretabilidade não só melhora os resultados, mas também garante confiança nas decisões, seja por parte de médicos, gestores ou clientes.</p>

<h2 id="o-fluxo-do-algoritmo">O Fluxo do Algoritmo</h2>
<p>Vamos entender, de forma simplificada, como o algoritmo dos autores funciona e qual o papel de cada etapa no processo.</p>

<h3 id="1-coleta-e-preparação-dos-dados">1. Coleta e Preparação dos Dados</h3>
<p>Tudo começa com a obtenção e preparação dos dados. No caso do artigo, os autores usaram dados clínicos sobre doenças cardíacas. Mas os princípios de tratamento de dados são aplicáveis a qualquer contexto de negócios. As etapas incluem:</p>

<ul>
  <li>
    <p><strong>Tratamento de valores ausentes</strong>: Dados faltantes podem comprometer a qualidade do modelo. Sejam substituídos pela média, moda ou outras técnicas, ou mesmo eliminados, isso precisa ser feito com cuidado.</p>
  </li>
  <li>
    <p><strong>Codificação de variáveis categóricas</strong>: Muitos modelos de aprendizado de máquina só funcionam com dados numéricos, então variáveis categóricas precisam ser transformadas, com técnicas como <em>One-Hot Encoding</em>.</p>
  </li>
  <li>
    <p><strong>Normalização ou padronização</strong>: Variáveis com escalas diferentes (como renda anual e idade) precisam ser ajustadas para facilitar o trabalho do algoritmo.</p>
  </li>
  <li>
    <p><strong>Divisão dos dados</strong>: Sempre dividimos os dados em treinamento e teste para garantir que possamos avaliar o modelo com dados que ele ainda não viu.</p>
  </li>
  <li>
    <p><strong>Balanceamento de classes</strong>: Quando as classes estão desbalanceadas (por exemplo, muito mais clientes inadimplentes do que adimplentes), técnicas como o <code class="language-plaintext highlighter-rouge">SMOTE</code> podem ser utilizadas para gerar amostras sintéticas da classe minoritária e garantir que o modelo seja treinado de forma equilibrada.</p>
  </li>
</ul>

<h3 id="2-seleção-dos-modelos-de-ensemble">2. Seleção dos Modelos de <em>Ensemble</em></h3>
<p>Os autores do artigo optaram por três modelos de <em>ensemble</em>, que são conhecidos por sua robustez e capacidade de reduzir o risco de <em>overfitting</em>:</p>

<ul>
  <li>
    <p><strong>Random Forest</strong>: Um conjunto de várias árvores de decisão, construídas a partir de amostras aleatórias dos dados. Cada árvore contribui com uma previsão, e o modelo final toma uma “decisão coletiva”. Isso reduz a variação e melhora a precisão.</p>
  </li>
  <li>
    <p><strong>AdaBoost</strong>: Este modelo ajusta várias iterações de classificadores simples, atribuindo mais peso aos exemplos mal classificados a cada nova rodada. Ele se “adapta” aos erros, ajustando-se para melhorar a precisão.</p>
  </li>
  <li>
    <p><strong>XGBoost</strong>: Uma implementação otimizada de <em>Gradient Boosting</em>, que se destaca pela eficiência e pelo desempenho elevado. É amplamente utilizado em competições de <em>machine learning</em> devido à sua capacidade de lidar com grandes volumes de dados e produzir previsões precisas.</p>
  </li>
</ul>

<h3 id="3-otimização-de-hiperparâmetros-com-otimização-bayesiana">3. Otimização de Hiperparâmetros com Otimização Bayesiana</h3>
<p>A cereja do bolo nesta abordagem é a <strong>Otimização Bayesiana</strong>, que torna o processo de ajuste de hiperparâmetros mais inteligente. Em vez de usar o <em>grid search</em> tradicional, que testa várias combinações de parâmetros de forma exaustiva, a Otimização Bayesiana modela a função objetivo (neste caso, a AUC) e decide de forma eficiente quais conjuntos de parâmetros testar a seguir. Isso economiza tempo e recursos computacionais.</p>

<h3 id="4-avaliação-e-seleção-do-modelo">4. Avaliação e Seleção do Modelo</h3>
<p>Depois que os modelos são otimizados, eles são avaliados em métricas como:</p>

<ul>
  <li>
    <p><strong>AUC (Área Sob a Curva ROC)</strong>: Mede a capacidade do modelo de distinguir entre classes positivas e negativas.</p>
  </li>
  <li>
    <p><strong>F1-Score</strong>: A média harmônica entre precisão e recall, usada principalmente quando os dados estão desbalanceados.</p>
  </li>
  <li>
    <p><strong>Precisão e Recall</strong>: Essas métricas medem a proporção de predições corretas e a capacidade do modelo de encontrar todas as instâncias da classe positiva.</p>
  </li>
</ul>

<p>Os autores ajustaram o limiar de decisão (<em>threshold</em>) para otimizar as métricas de acordo com o contexto, um passo importante quando o equilíbrio entre precisão e recall é crucial.</p>

<h3 id="5-interpretabilidade-com-valores-shap">5. Interpretabilidade com Valores SHAP</h3>
<p>Uma vez escolhido o melhor modelo, os autores usaram os valores SHAP (<em>SHapley Additive ExPlanations</em>) para explicar as previsões. O SHAP nos ajuda a entender a importância de cada característica, atribuindo um valor específico para a contribuição de cada variável nas previsões do modelo.</p>

<p>Por que isso é importante? Porque em muitos casos, como em diagnósticos médicos ou decisões empresariais, o “porquê” é tão importante quanto o resultado em si. Saber que uma variável como “pressão arterial” influenciou fortemente a decisão pode ser crucial para validar o resultado e tomar decisões informadas.</p>

<h2 id="por-que-essa-abordagem-é-importante">Por Que Essa Abordagem É Importante?</h2>
<p>Esse tipo de abordagem, que combina precisão com interpretabilidade, é crucial em muitos contextos. Modelos eficientes são ótimos, mas se você não souber explicar suas decisões, pode perder a confiança de seus <em>stakeholders</em>. Isso é especialmente relevante em setores como saúde e finanças, onde a transparência é uma exigência.</p>

<p>Além disso, o uso da <strong>Otimização Bayesiana</strong> ajuda a economizar tempo e recursos, ajustando os modelos de maneira eficiente e inteligente. Isso significa menos horas gastas testando combinações de hiperparâmetros e mais tempo para analisar e melhorar os resultados.</p>

<h2 id="adaptando-para-outros-contextos">Adaptando para Outros Contextos</h2>
<p>Embora o foco do artigo seja a previsão de doenças cardíacas, os mesmos princípios podem ser aplicados em vários outros cenários de negócios:</p>

<ul>
  <li>
    <p><strong>Análise de risco de crédito</strong>: Prever a probabilidade de inadimplência de clientes.</p>
  </li>
  <li>
    <p><strong>Detecção de fraudes</strong>: Identificar transações suspeitas em tempo real.</p>
  </li>
  <li>
    <p><strong>Previsão de <em>churn</em></strong>: Antecipar quais clientes estão propensos a abandonar o serviço.</p>
  </li>
  <li>
    <p><strong>Classificação de <em>leads</em></strong>: Priorizar potenciais clientes com maior probabilidade de conversão.</p>
  </li>
</ul>

<p>A estrutura do algoritmo permanece a mesma; o que muda são os dados e, possivelmente, algumas nuances no pré-processamento ou nas métricas de avaliação.</p>

<h2 id="um-vislumbre-do-código">Um Vislumbre do Código</h2>
<p>Para ilustrar como isso se traduz em código <strong>Python</strong>, vamos dar uma olhada em um esboço simplificado da implementação:</p>

<pre><code class="language-{python}"># Importação das bibliotecas necessárias
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from skopt import BayesSearchCV
import shap
from sklearn.datasets import fetch_openml
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Carregamento e preparação dos dados com as_frame=False
data = fetch_openml(name='heart', version=1, as_frame=False)

# Definir os nomes das colunas conforme documentação do dataset
feature_names = [
    "age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", "thalach", 
    "exang", "oldpeak", "slope", "ca", "thal"
]

# Converta a matriz esparsa para uma matriz densa e crie o DataFrame
X = pd.DataFrame(data.data.toarray(), columns=feature_names)  # Converte para DataFrame com os nomes corretos
y = pd.Series(data.target)

# Certifique-se de que a variável-alvo é binária (0 ou 1)
y = np.where(y &gt; 0, 1, 0)  # Converte todas as classes maiores que 0 para 1

# Divisão em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Normalizar as variáveis numéricas
# Ajuste para lidar com dados esparsos, com with_mean=False
scaler = StandardScaler(with_mean=False)
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Definição do espaço de hiperparâmetros para o Random Forest
param_space = {
    'n_estimators': (10, 200),
    'max_depth': (1, 20),
    'min_samples_split': (2, 10)
}

# Otimização Bayesiana
bayes_search = BayesSearchCV(
    RandomForestClassifier(random_state=42),
    param_space,
    n_iter=100,
    scoring='roc_auc',
    cv=5,
    n_jobs=-1,
    random_state=42
)

# Ajuste do modelo
bayes_search.fit(X_train, y_train)
best_model = bayes_search.best_estimator_

# Avaliação do modelo
y_pred = best_model.predict(X_test)

# Calcular a acurácia e exibir a matriz de confusão
accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

print(f"Acurácia: {accuracy:.4f}")
print("Matriz de Confusão:")
print(cm)

# Verifique se X_test é uma matriz densa
X_test_dense = pd.DataFrame(X_test, columns=feature_names)

# Interpretabilidade com SHAP
explainer = shap.TreeExplainer(best_model)
shap_values = explainer.shap_values(X_test_dense)

# Pegando os valores SHAP apenas para a classe 1 (classe positiva)
shap_values_class_1 = shap_values[:, :, 1]

# Gerar o gráfico com os valores SHAP para a classe positiva
shap.summary_plot(shap_values_class_1, X_test_dense, show=False)

# Salvar o gráfico gerado em PNG
#plt.savefig('shap_summary_plot.png', format='png', dpi=300)

# Exibir o gráfico (se desejar vê-lo na tela também)
plt.show()
</code></pre>

<p>Aqui, aplicamos a Otimização Bayesiana para encontrar os melhores hiperparâmetros para um modelo de <em>Random Forest</em> e usamos o SHAP para interpretar suas previsões.</p>

<div style="text-align: center;">
  <img src="/assets/images/shap_summary_plot_01-01.png" alt="Figura 1.1: Interpretabilidade com SHAP" style="width: 75%" />
  <div style="font-style: italic; font-size: 0.95em;">1.1: Figura 1.1: Interpretabilidade com SHAP.</div>
</div>
<p><br /></p>

<div style="text-align: center;">
  <img src="/assets/images/roc_curve_01-02.png" alt="Figura 1.2: Curva ROC" style="width: 75%" />
  <div style="font-style: italic; font-size: 0.95em;">1.2: Figura 1.2: Curva ROC.</div>
</div>
<p><br /></p>

<div style="text-align: center;">
  <img src="/assets/images/precisao_recall_curve_01-03.png" alt="Figura 1.3: Curva Precisão - Recall" style="width: 75%" />
  <div style="font-style: italic; font-size: 0.95em;">1.3: Figura 1.3: Curva Precisão - Recall.</div>
</div>
<p><br /></p>

<pre><code class="language-{python}">from sklearn.metrics import accuracy_score, confusion_matrix

# Convertendo as probabilidades em classes binárias usando um threshold (por exemplo, 0.5)
y_pred = (y_pred_proba &gt;= 0.5).astype(int)

# Cálculo da acurácia
accuracy = accuracy_score(y_test, y_pred)
print(f"Acurácia: {accuracy:.4f}")

# Geração da matriz de confusão
cm = confusion_matrix(y_test, y_pred)
print("Matriz de Confusão:")
print(cm)
</code></pre>

<pre><code class="language-{python}">Acurácia: 0.8642
Matriz de Confusão:
[[39  6]
 [ 5 31]]
</code></pre>

<h2 id="comentando-as-figuras-e-resultados">Comentando as Figuras e Resultados</h2>

<p>Para complementar nossa compreensão inicial, vamos dar uma olhada rápida nas figuras e nos resultados obtidos pelo modelo. Não se preocupe se alguns conceitos ainda não estiverem claros; nos próximos capítulos, iremos explorar cada um deles em detalhes.</p>

<h3 id="figura-11-interpretabilidade-com-shap">Figura 1.1: Interpretabilidade com SHAP</h3>
<p>A <strong>Figura 1.1</strong> apresenta um gráfico de resumo dos valores SHAP, que nos permite visualizar a importância de cada característica no modelo. As cores e posições dos pontos fornecem <em>insights</em> sobre como cada variável afeta as previsões. Embora não entremos em detalhes agora, este gráfico é fundamental para entender a interpretabilidade do modelo, e iremos explorá-lo aprofundadamente mais adiante.</p>

<h3 id="figura-12-curva-roc">Figura 1.2: Curva ROC</h3>
<p>A <strong>Figura 1.2</strong> mostra a Curva ROC (<em>Receiver Operating Characteristic</em>) do nosso modelo. Esta curva é uma ferramenta poderosa para avaliar a capacidade do modelo em distinguir entre as classes positivas e negativas em diferentes limiares de decisão. A proximidade da curva ao canto superior esquerdo indica um desempenho robusto. Nos capítulos futuros, iremos dissecar esta curva para entender completamente o que ela revela sobre a performance do modelo.</p>

<h3 id="figura-13-curva-precisão---recall">Figura 1.3: Curva Precisão - Recall</h3>
<p>A <strong>Figura 1.3</strong> apresenta a Curva Precisão-Recall, que é especialmente útil quando lidamos com conjuntos de dados desbalanceados. Esta curva nos ajuda a encontrar o equilíbrio ideal entre precisão e recall, permitindo otimizar o modelo de acordo com as necessidades específicas do negócio. Fique tranquilo, iremos explorar como interpretar e utilizar esta curva nos próximos capítulos.</p>

<h3 id="resultados-de-desempenho-do-modelo">Resultados de Desempenho do Modelo</h3>
<p>Além das visualizações geradas com SHAP e as curvas de ROC e Precisão-Recall, obtivemos métricas quantitativas que ajudam a avaliar o desempenho do modelo:</p>

<ul>
  <li>
    <p><strong>Acurácia</strong>: 0.8642</p>
  </li>
  <li>
    <p><strong>Matriz de Confusão</strong>:</p>
  </li>
</ul>

<pre><code class="language-{python}">    [[ 39   6]
     [  5 31]]
</code></pre>

<p>A acurácia indica que nosso modelo está correto em aproximadamente 86% das previsões, o que é um resultado promissor. A matriz de confusão nos fornece uma visão detalhada das classificações verdadeiras e falsas para cada classe:</p>

<ul>
  <li>
    <p><strong>Verdadeiros Positivos (TP)</strong>: 39 casos em que o modelo previu positivamente e estava correto.</p>
  </li>
  <li>
    <p><strong>Verdadeiros Negativos (TN)</strong>: 30 casos em que o modelo previu negativamente e estava correto.</p>
  </li>
  <li>
    <p><strong>Falsos Positivos (FP)</strong>: 6 casos em que o modelo previu positivamente, mas estava incorreto.</p>
  </li>
  <li>
    <p><strong>Falsos Negativos (FN)</strong>: 5 casos em que o modelo previu negativamente, mas estava incorreto.</p>
  </li>
</ul>

<p>Este balanço entre TP, TN, FP e FN é crucial para entender as implicações práticas do modelo, especialmente em contextos onde os custos de falsos positivos e falsos negativos são diferentes. No decorrer do livro, iremos analisar profundamente estes resultados para extrair <em>insights</em> valiosos e aprimorar ainda mais o modelo.</p>

<p>Este é apenas um esboço para dar uma ideia geral. Nos próximos capítulos, vamos aprofundar cada etapa, adicionando detalhes e explicações para que você possa replicar e adaptar a abordagem ao seu contexto específico.</p>

<p>Neste capítulo, exploramos em detalhes o algoritmo proposto pelos autores do artigo que inspirou nossa jornada. Entendemos como a combinação de modelos de <em>ensemble</em> otimizados e interpretabilidade através de valores SHAP pode criar soluções poderosas e aplicáveis a diversos problemas de negócios.</p>

<p>A chave aqui é a adaptabilidade. Embora o caso de uso original seja a previsão de doenças cardíacas, os princípios e técnicas podem ser aplicados a uma ampla gama de desafios. Compreender o fluxo do algoritmo e os motivos por trás de cada etapa nos prepara para colocar tudo isso em prática.</p>

<p>No próximo capítulo, vamos arregaçar as mangas e mergulhar no código. Vamos apresentar a implementação completa, explicando cada linha em detalhes para que você possa entender não apenas o “como”, mas também o “porquê” de cada decisão.</p>

<p>Prepare seu ambiente de desenvolvimento e até lá!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Aqui, vamos mergulhar no algoritmo que deu origem a toda essa aventura. Sim, estamos falando daquele artigo com título complicado: “Optimized Ensemble Learning Approach with Explainable AI for Improved Heart Disease Prediction”, publicado em junho de 2024. Embora não seja um assunto trivial, com um pouco de esforço e dedicação, você verá que é perfeitamente compreensível e, mais importante, aplicável a problemas reais de negócios.]]></summary></entry></feed>